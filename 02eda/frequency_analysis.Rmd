---
title: "Analysis of N-Gram Frequencies"
author: "Ivan Lysiuchenko"
date: "August 19, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r echo=FALSE, message=FALSE}
library(quanteda)
library(ggplot2)
```

## Extracting Frequency Information

We suppose that the document-feature matrices for our corpora are already computed
and stored in a specific place (see the code).

This is the function we use to extract the vector of frequencies. Frequencies are sorted 
in descending order.

```{r message=FALSE}
loadFrequencies <- function(pathName, freqPathname, update = FALSE)
{
    if (!file.exists(freqPathname) || update)
    {
        matrCon <- file(pathName, "r")
        partMatr <- unserialize(matrCon)
        close(matrCon)
        # group all the rows, just in case
        partMatr <- dfm_group(partMatr, groups = rep(1, nrow(partMatr)))
        # sort the features by frequency
        partMatr <- dfm_sort(partMatr);
        
        freqs <- colSums(partMatr)
        freqCon <- file(freqPathname, "w")
        serialize(freqs, freqCon)
        close(freqCon)
        
        freqs
    }
    else
    {
        freqCon <- file(freqPathname, "r")
        freqs <- unserialize(freqCon)
        close(freqCon)
        
        freqs
    }
}
```

## Frequencies of words, bigrams and trigrams in the entire corpus

Here we explore how frequently different terms occur in the merged corpus
comprising the texts from blogs, news and twits.
Let's take a look at words first.

```{r message=FALSE, echo=FALSE}
freqs <- loadFrequencies("../results/dfm/generalDfm1.dat", "../results/dfm/freq1.dat")
#freqCon <- file("../results/dfm/freq1.dat", "w")
#serialize(freqs, freqCon)
#close(freqCon)

# Look for 50% and 90%
threshold50 <- 0.5 * sum(freqs)
threshold90 <- 0.9 * sum(freqs)

sufficient50 <- sum(cumsum(freqs) < threshold50) + 1
sufficient90 <- sum(cumsum(freqs) < threshold90) + 1

#dsize <- length(freqs)
dsize <- min(sufficient90 + 100, length(freqs))
g <- ggplot(data = data.frame(x = log(1:dsize), y = log(freqs[1:dsize])), 
                                    mapping = aes(x = x, y = y, color = "red")) + 
    geom_line() +
    geom_vline(xintercept = log(sufficient50), color = "red") + 
    geom_vline(xintercept = log(sufficient90), color = "red") +
    labs(x = "log(num. feature)", y = "log(count)")
```

Our preprocessing step detected `r length(freqs)` different words. 
All of them account for `r sum(freqs)` entries.
The most frequent words are the following:

```{r echo=FALSE}
head(freqs, 20)
```

Examples of unfrequent words:

```{r echo=FALSE}
tail(freqs, 10)
```

It turns out that to cover 90% of all word usages we need to keep `r sufficient90` of the most frequent words.
50% is covered by `r sufficient50` of them.


```{r message=FALSE, echo=FALSE}
freqs <- loadFrequencies("../results/dfm/generalDfm2.dat", "../results/dfm/freq2.dat")

# Look for 50% and 90%
threshold50 <- 0.5 * sum(freqs)
threshold90 <- 0.9 * sum(freqs)

sufficient50 <- sum(cumsum(freqs) < threshold50) + 1
sufficient90 <- sum(cumsum(freqs) < threshold90) + 1

#dsize <- length(freqs)
dsize <- min(sufficient90 + 100, length(freqs))
#g <- ggplot(data = data.frame(x = 1:dsize, y = log(freqs[1:dsize])), 
#                                    mapping = aes(x = x, y = y), color = "blue") + 
#    geom_line() +
#    geom_vline(xintercept = sufficient50) + geom_vline(xintercept = sufficient90)

g <- g + geom_line(data = data.frame(x = log(1:dsize), y = log(freqs[1:dsize])), 
                                    mapping = aes(x = x, y = y), color = "blue") +
    geom_vline(xintercept = log(sufficient50), color = "blue") + 
    geom_vline(xintercept = log(sufficient90), color = "blue")
```

The number of different bigrams is `r length(freqs)`, the overall number of their entries is `r sum(freqs)`.
90% and 50% of the entries correspond to `r sufficient90` and `r sufficient50` most frequent bigrams.
The most frequent bigrams are the following:

```{r  message=FALSE, echo=FALSE}
head(freqs, 20)
```

Examples of unfrequent bigrams:

```{r message=FALSE, echo=FALSE}
tail(freqs, 10)
```

```{r message=FALSE, echo=FALSE}
freqs <- loadFrequencies("../results/dfm/generalDfm3.dat", "../results/dfm/freq3.dat")

## Look for 50% and 90%
#threshold50 <- 0.5 * sum(freqs)
#threshold90 <- 0.9 * sum(freqs)
#
#sufficient50 <- sum(cumsum(freqs) < threshold50) + 1
#sufficient90 <- sum(cumsum(freqs) < threshold90) + 1

#dsize <- length(freqs)
# at the moment, the sufficient90 for bigrams
dsize <- min(sufficient90 + 100, length(freqs))

g <- g + geom_line(data = data.frame(x = log(1:dsize), y = log(freqs[1:dsize])), 
                                    mapping = aes(x = x, y = y), color = "green")
```

At the preprocessing step we filtered out the less frequent trigrams.
Now we keep `r length(freqs)` of different features. These features account for `r sum(freqs)` entries.
The most frequent trigrams are the following:

```{r  message=FALSE, echo=FALSE}
head(freqs, 20)
```

Among our fitered trigrams, these are the less frequent:

```{r message=FALSE, echo=FALSE}
tail(freqs, 10)
```


```{r message=FALSE, echo=FALSE}
freqs <- loadFrequencies("../results/dfm/generalDfm4.dat", "../results/dfm/freq4.dat")

## Look for 50% and 90%
#threshold50 <- 0.5 * sum(freqs)
#threshold90 <- 0.9 * sum(freqs)
#
#sufficient50 <- sum(cumsum(freqs) < threshold50) + 1
#sufficient90 <- sum(cumsum(freqs) < threshold90) + 1

#dsize <- length(freqs)
# at the moment, the sufficient90 for bigrams
dsize <- min(sufficient90 + 100, length(freqs))

g <- g + geom_line(data = data.frame(x = log(1:dsize), y = log(freqs[1:dsize])), 
                                    mapping = aes(x = x, y = y), color = "black")
```

At the preprocessing step we filtered out the less frequent 4-grams.
Now we keep `r length(freqs)` of different features. These features account for `r sum(freqs)` entries.
The most frequent 4-grams are the following:

```{r  message=FALSE, echo=FALSE}
head(freqs, 20)
```

Among our fitered 4-grams, these are the less frequent:

```{r message=FALSE, echo=FALSE}
tail(freqs, 10)
```


Let's show the frequencies of the words, bigrams and trigrams on the same graph,
using a logarithmic scale.

```{r echo=FALSE}
rm(freqs)
g
```